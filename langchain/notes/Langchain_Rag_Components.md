# LangChain의 주요 RAG 컴포넌트

## 동작 원리
1. **문서 읽기**  
   - 외부 자료를 다양한 형식으로 로드합니다.
2. **컨텍스트 생성**  
   - 로드된 데이터 중에서 질문에 적합한 부분을 검색하여 컨텍스트를 만듭니다.
3. **프롬프트 작성**  
   - LLM의 사용자 쿼리와 컨텍스트를 결합하여 최적의 프롬프트를 작성합니다.
4. **답변 생성**  
   - 작성된 프롬프트를 LLM에 전달하고 결과를 출력합니다.

---

### 문서 로더 (Document Loaders)
- **다양한 형식의 문서를 로드**  
  - `TextLoader`: 일반 텍스트 파일 읽기  
  - `PDFLoader`: PDF 파일 로드  
  - `CSVLoader`: CSV 형식 데이터 로드  
  - 추가적으로 HTML, JSON 등도 지원 가능
- **사용 사례**  
  - 대량의 문서 기반 질의응답 시스템 구축  
  - 구조화된 데이터를 비정형 텍스트 데이터로 변환  

---

### 텍스트 분할기 (Text Splitters)
- **문서를 처리 가능한 크기로 분할**  
  - LLM이 처리 가능한 토큰 수 제한을 고려하여 적절한 청크 크기로 나눔.
  - 예를 들어, OpenAI GPT-3는 최대 4096개의 토큰 제한이 있음.
- **분할 전략**  
  - 문자 기반 분할: 문자의 개수를 기준으로 청크 분할.  
  - 토큰 기반 분할: LLM의 토큰화 방식을 고려하여 텍스트를 분할.  
  - 문단 기반 분할: 문서의 문단 구조를 유지하며 분할.  
- **장점**  
  - 효율적인 검색: 질문에 적합한 컨텍스트만 추출 가능.  
  - 처리 속도 향상: 불필요한 데이터를 제외하고 LLM에 전달.

---

### 임베딩 모델 (Embeddings)
- **텍스트 -> 벡터 변환**  
  - 문서를 읽고, 의미와 맥락을 벡터(숫자)로 표현.
  - 모델별로 **임베딩 차원**이 다름.  
    - 고차원: 더 풍부한 표현, 높은 계산 비용.  
    - 저차원: 빠른 연산, 낮은 표현력.
- **RAG에서 중요한 역할**  
  - 벡터를 기반으로 유사도를 측정하여 검색 정확도 향상.
- **LangChain에서 제공하는 Embeddings 클래스**  
  - `embed_documents`: 여러 텍스트를 한 번에 임베딩.  
  - `embed_query`: 단일 쿼리 텍스트를 임베딩.  
  - 모델 지원: OpenAI, Hugging Face Transformers 등.

#### 활용 사례
1. **의미 기반 검색 (Semantic Search)**  
   - 사용자의 쿼리와 유사한 문서를 벡터 유사도로 검색.  
2. **텍스트 유사도 분석**  
   - 표절 검사, 중복 문서 탐지, 감성 분석.  
3. **문서 분류**  
   - 문서의 주제를 기반으로 카테고리 분류.  
4. **추천 시스템**  
   - 사용자 관심사와 유사한 콘텐츠 추천.

---

### 벡터 저장소 (Vector Stores)
- **임베딩된 벡터를 저장하고 검색하는 데이터베이스**  
  - 검색 속도 향상을 위해 사용.
- **주요 동작**  
  1. 입력된 텍스트를 벡터로 변환.  
  2. 저장된 벡터와 비교하여 가장 유사한 결과 반환.
- **오픈소스 도구**  
  - **Chroma**: 사용이 간단하며 LangChain과 잘 통합.  
  - **FAISS**: 대규모 데이터 처리와 GPU 가속 지원.  
  - **Pinecone, Weaviate**: 상용 서비스로 대규모 데이터를 안정적으로 처리.

---

### 검색기 (Retrievers)
- **질의와 관련된 문서를 검색하는 컴포넌트**  
  - 벡터 저장소와 LLM을 연결하는 핵심 도구.
- **LangChain의 통합 방식**  
  - `as_retriever` 메소드를 사용하여 RAG 체인에 통합 가능.

---

### 언어 모델 (LLMs)
- **텍스트 생성의 핵심**  
  - 사용자 입력 및 검색된 컨텍스트를 기반으로 답변 생성.
- **RAG 체인 동작**  
  1. 사용자 입력을 기반으로 검색.  
  2. 검색된 결과와 사용자 입력을 결합한 프롬프트 생성.  
  3. 프롬프트를 LLM에 전달하여 최적의 답변 생성.  
- **모델 지원**  
  - OpenAI GPT, Google Bard, Hugging Face 모델 등.

---

LangChain의 RAG 컴포넌트는 문서 로드, 임베딩, 검색, 생성을 유기적으로 연결하여 효율적인 정보 검색과 응답 생성을 지원합니다. 이를 통해 다양한 데이터 활용 사례에 적용할 수 있습니다.
