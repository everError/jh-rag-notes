# RAG 검색 시스템 평가 및 데이터셋 준비

RAG(Retrieval-Augmented Generation) 시스템의 성능을 평가하기 위해서는 적절한 데이터셋을 준비하고, 다양한 평가 방법을 적용해야 합니다. 이 문서에서는 RAG 검색 시스템 평가를 위한 데이터셋 구성 방법과 평가 지표에 대해 다룹니다.

---

## 1. 검색 시스템 평가를 위한 데이터셋 준비

### 1.1 데이터셋 구성 방법

RAG 시스템 평가를 위해서는 **문서(지식 베이스)**, **질문(쿼리)**, **정답(기대 출력)** 세 가지 요소가 필요합니다.

### **데이터셋 구성 흐름도**

```
┌──────────────────────────┐
│   데이터 수집 단계        │
├──────────────────────────┤
│ 1. 도메인별 문서 수집      │
│ 2. 질문 데이터 생성       │
│ 3. 정답(기대 출력) 생성   │
└──────────────────────────┘
        │
        ▼
┌──────────────────────────┐
│    데이터 전처리 단계      │
├──────────────────────────┤
│ 1. 중복 데이터 제거       │
│ 2. 문서 클렌징 및 필터링  │
│ 3. 메타데이터 추가        │
└──────────────────────────┘
        │
        ▼
┌──────────────────────────┐
│  검색 평가 데이터 구축 단계 │
├──────────────────────────┤
│ 1. 검색 가능 형태로 저장   │
│ 2. 임베딩 및 벡터 변환    │
│ 3. 평가용 쿼리 매칭      │
└──────────────────────────┘
```

#### **1. 도메인별 문서 수집**

- 뉴스 기사, 위키피디아 문서, 기술 문서, 논문 등 다양한 데이터 출처 활용
- 도메인에 특화된 문서를 구축하여 실제 적용 사례에 맞춤형 평가 가능
- 데이터의 최신성을 유지하기 위해 주기적인 업데이트 필요

#### **2. 질문 데이터 생성**

- 기존 Q&A 데이터셋 활용 (예: SQuAD, Natural Questions, TriviaQA)
- LLM을 활용한 질문 생성 (자동화 가능)
- 수동 검토를 통해 질문의 품질과 유효성 확보
- 질의 유형: 사실 질문(Fact-based), 추론 질문(Inference-based), 다중 선택형(Multiple choice)

#### **3. 정답(기대 출력) 생성**

- 원본 문서에서 직접 답변 추출
- LLM을 활용하여 답변 생성 후 전문가 검토
- 여러 개의 정답을 허용하는 유연한 평가 구조 마련
- 평가 시 다중 정답(Multiple Answers) 고려하여 점수 계산 가능

#### **4. LLM 기반 생성 데이터의 검증**

LLM을 활용하여 생성한 질문과 정답을 검증하는 과정이 필수적이며, 크게 **자동 검증(Auto Evaluation)**과 **수동 검증(Human Evaluation)** 방법이 있습니다.

#### **4.1 자동 검증 (Auto Evaluation)**

자동 검증 방법은 LLM 또는 평가 모델을 활용하여 데이터 품질을 검토하는 방식입니다.

1. **정확도 비교 (Text Similarity Metrics)**

   - BLEU, ROUGE, METEOR 등의 자연어 평가 지표 활용
   - 정답과 LLM이 생성한 답변의 텍스트 유사도를 분석하여 평가

2. **정확성 평가 (Fact Verification)**

   - LLM을 이용하여 정답의 정확도를 재확인
   - 지식 기반 QA 모델(예: OpenAI API, Hugging Face 모델) 활용 가능

3. **일관성 평가 (Consistency Checking)**
   - 동일한 질문을 여러 번 실행하여 결과가 일관적인지 평가
   - 과도한 변동성을 가지는 질문은 데이터셋에서 제거

```python
from langchain.evaluation import QAEvaluator

# 평가 데이터셋 준비
predictions = ["파리는 프랑스의 수도입니다.", "프랑스의 수도는 파리입니다."]
ground_truths = ["프랑스의 수도는 파리입니다."]

evaluator = QAEvaluator()
score = evaluator.evaluate(predictions, ground_truths)
print("Evaluation Score:", score)
```

#### **4.2 수동 검증 (Human Evaluation)**

수동 검증은 사람이 직접 생성된 질문과 정답의 품질을 평가하는 방식으로, 다음과 같은 평가 기준을 활용합니다.

1. **정확성 (Accuracy)**

   - 답변이 문서의 정보와 일치하는지 평가

2. **유효성 (Relevance)**

   - 질문이 문맥에 맞는 유효한 질문인지 확인

3. **자연스러움 (Fluency & Readability)**

   - 문장의 문법이 자연스럽고 의미가 명확한지 검토

4. **편향성 (Bias Detection)**

   - 질문과 정답이 편향된 내용을 포함하고 있지 않은지 점검

5. **중복 제거 (Redundancy Check)**
   - 동일한 질문이나 정답이 반복되지 않도록 필터링

```python
# 수동 검토 결과를 JSON으로 저장하는 예제
import json

evaluation_results = {
    "question": "프랑스의 수도는 어디인가?",
    "generated_answer": "파리입니다.",
    "human_review": {
        "accuracy": "높음",
        "relevance": "적절함",
        "fluency": "자연스러움",
        "bias": "없음",
        "redundancy": "중복 없음"
    }
}

with open("evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(evaluation_results, f, ensure_ascii=False, indent=4)
```

### 1.2 데이터셋 준비 방법별 장단점 비교

| 데이터셋 준비 방법              | 장점                                          | 단점                                           |
| ------------------------------- | --------------------------------------------- | ---------------------------------------------- |
| 공개 Q&A 데이터셋 사용          | 데이터 품질이 검증됨, 다양한 도메인 적용 가능 | 특정한 맞춤형 도메인에는 적합하지 않을 수 있음 |
| LLM을 이용한 자동 질문 생성     | 빠르게 대량의 데이터 생성 가능                | 노이즈가 많을 수 있으며 품질 보장이 어려움     |
| 전문가가 직접 질문 및 답변 생성 | 높은 품질의 평가 데이터 구축 가능             | 시간과 비용이 많이 듦                          |

---

## 2. 검색 시스템 평가 방법 및 지표

### 2.1 검색 평가 지표 (Information Retrieval Metrics)

1. **Hit Rate (적중률)**
   - 검색된 문서 목록 내에 관련 문서가 포함될 확률
   - 값이 높을수록 검색 성능이 우수함을 의미
2. **MRR (Mean Reciprocal Rank)**

   - 첫 번째 관련 문서가 검색된 순위의 역수를 평균한 값
   - 높은 값일수록 사용자가 첫 번째 페이지에서 원하는 정보를 찾을 확률이 높음

   $$MRR = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{rank_i}$$

3. **Recall@k**

   - 상위 k개의 검색 결과 중 실제 관련 문서가 포함된 비율
   - 모든 관련 문서를 포함할 확률을 평가하는 지표

4. **Precision@k**

   - 상위 k개의 검색 결과 중 관련 문서의 비율
   - 검색 결과의 정확도를 나타냄

5. **mAP (Mean Average Precision)**

   - 여러 쿼리에 대해 Precision 값을 평균하여 검색 시스템의 전체적인 성능 평가
   - 검색된 관련 문서가 상위에 위치할수록 높은 점수 부여

6. **NDCG (Normalized Discounted Cumulative Gain)**

   - 검색된 문서의 순위와 관련성을 모두 고려하는 지표
   - 높은 관련성 문서가 상위에 위치할수록 점수가 높아짐

   $$NDCG@k = \frac{DCG@k}{IDCG@k}$$

   $$DCG@k = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i+1)}$$

   $$IDCG@k = \text{최상의 순서로 정렬된 DCG}$$

---

## 3. 결론

- 검색 시스템 평가를 위해 **적절한 데이터셋 구성**이 중요하며, 도메인에 따라 **질문과 답변 생성 방법을 조정**해야 함.
- 평가 지표로는 **MRR, Recall@k, Precision@k, NDCG** 등이 활용되며, 검색 시스템의 성능을 다양한 각도에서 분석할 수 있음.
- **LLM을 활용한 질문 및 정답 데이터 검증**이 필요하며, **자동 평가 및 수동 평가를 병행하여 데이터 품질을 보장**해야 함.
- **평가 자동화**를 통해 RAG 모델을 지속적으로 개선할 수 있으며, LangChain과 같은 도구를 활용하면 효율적인 평가 및 최적화가 가능함.

---

이 문서는 RAG 검색 시스템의 평가 및 데이터셋 준비에 대한 가이드를 제공하며, 실제 적용 시 다양한 방법을 조합하여 최적의 성능을 도출하는 것이 중요합니다.
